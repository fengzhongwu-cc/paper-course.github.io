

## 30. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

### 30.1 第一课时

<video width=80%  controls >
	<source type="video/mp4" src="030-pre-training-of-deep-bidirectional-transformers-for-Language-understanding/030-1.mp4">
</video>

**任务名称：**了解机器翻译的基本概念和无监督机器翻译的相关技术

**任务简介：**观看视频第一课时，阅读论文

**任务详解：**

1. 视频第一课时
   1. 机器翻译

      语言模型是根据语言客观事实而进行的语言抽象数学建模，是一个单纯的、统一的、抽象的形式系统。语言客观事实经过语言模型的描述，比较适合于电子计算机进行自动处理。简单来说，语言模型就是出现一串单词序列的概率分布，是为一个长度为 n 的文本确定一个概率分布 p，来表示这段文本存在的可能性，更通俗一点，也就是在度量这个单词序列是不是人话。

   2. 从语言模型到Word Embedding

      最初，Word Embedding是语言模型的副产物

   3. 从Word Embedding到BERT

      Bert其实和ELMo及GPT存在千丝万缕的关系：

      - 如果我们把GPT的预训练阶段换成双向语言模型，那么就得到了Bert
      - 如果我们把ELMo的特征抽取器换成Transformer，那么也得到了Bert

      可以看出，Bert最关键有两点，第一点是特征抽取器采用Transformer，第二点是预训练的时候采用双向语言模型。。

2. 论文阅读

   花费大致4小时来阅读论文，主要通过阅读了解模型的大致结构，将疑问记录在作业中。

   这个部分我们只需要比较熟悉地了解到导读视频的程度，不需要过于深入地去学习，将重点放置在论文的模型上，对于其他的部分有疑问可以暂时记录着，并写在这两天的作业中。

**论文原文下载：**

链接：https://pan.baidu.com/s/1pLU8FFn_0qgm05b0ENwtpA 

提取码：ppht

**打卡要求：**（形式：文字，字数至少100）按照自己的理解总结上述三个知识点（最好分点清晰，有树状的层次结构，必要时推导公式）。

### 30.2 第二课时

<video width=80%  controls >
	<source type="video/mp4" src="030-pre-training-of-deep-bidirectional-transformers-for-Language-understanding/030-2.mp4">
</video>

**任务名称：**学习阅读论文的方法，进一步了解模型的部分细节，从理论掌握模型结构，论文中的技巧

**任务简介：**观看视频第二课时，阅读论文

**任务详解：**

1. 视频
   1. 学习阅读论文的方法

      论文的阅读不是从头到尾，也不是每个部分都一定是非常值得推敲。我们需要掌握练习合适的阅读节奏，在不同的学习阶段，去从论文中汲取不同的东西，将保证我们更有效率地学习，也将保证我们学习的过程更加自然。

   2. 进一步了解经典模型的部分细节

      经典模型的部分细节的了解可以帮助我们更好地理解论文模型的部分细节，同时也为大家自助学习其他的经典模型提供了一个铺垫。

   3. 从理论掌握模型结构，论文中的技巧

      模型的掌握不是画出网络图形，然后把公式写出来就算掌握了，那样只能算是对学习知识的复述。需要能真正明白模型的各个部分的所以然。这个部分将从整体到细节讲解模型结构的各个部分，主要思想的动机，与其他经典模型的对比，论文实现过程中使用到的技巧。

2. 阅读论文

   在学习完视频之后再阅读论文，看看自己是不是能了然于胸，对照课程PPT阅读，记录疑问

**打卡要求：**（形式：文字或者图片，字数至少100）

1. 回答下列问题：
   1. BERT的预训练中，预测下一个句子的目的是什么？
   2. 说出BERT和GPT的不同，至少三点？

2. 关闭一切资源，从零开始，画出BERT用于不同下游任务的改造施工图。

 