## 2. Efficient Estimation of Word Representations in Vector Space

### 2.1 第一课时

<video width=80%  controls >
	<source type="video/mp4" src="002-efficient-estimation-of-word-representations-in-vector-space/002-1.mp4">
</video>

任务名称：预习论文《Efficient Estimation of Word Representations in Vector Space》，了解论文领域的一些基础概念

任务简介：泛读整篇论文，学习视频第一课时-论文导读部分

详细说明：本节将会向大家介绍语言模型的概念及计算方式，除此之外为了方便理解，我们还引入n-gram模型；之后，还介绍了词向量的概念和种类，便于大家在接下来的学习word2vec中脑子中有更宏观的印象；此外，还会简单介绍一下在学习本期课程之前需要做的一些知识储备，以便同学们提前学习；最后还介绍一下本期课程的学习规划及对同学们在学习中的一些建议，以便让大家更有计划的学习。

本节内容主要有4大部分，两个研究背景，两个课程要求。

预习工作是正式学习前必要也是重要的一环，充分的预习能够在后续的学习中起到事半功倍的效果。在这个过程中，同学们首先要了解词向量是什么，有什么种类，以及生成词向量的方式有哪些。

本阶段首先要泛读论文，对论文中自己认为的重点或者不懂的难点要及时记录，遇到问题可以不去深究，待课程学习完再回顾解决。之后结合视频课程写学习笔记，完成论文预习任务。

论文资料下载：

链接：https://pan.baidu.com/s/1LOWDTaEu6HE1UMJBKOePMQ 

提取码：jhmf 

打卡要求：（形式：文字，字数，共150字以上）

1. 简述几种你了解的词向量。

2. 简述分布式词向量的优点。
3. 列举三个学习完本课程的问题。



### 2.2 第二课时

<video width=80%  controls >
	<source type="video/mp4" src="002-efficient-estimation-of-word-representations-in-vector-space/002-2.mp4">
</video>

任务名称：论文精读，学习论文的算法原理，掌握算法的流程和实现方式

任务简介：精读整篇论文，学习视频第二课时-论文精读部分

详细说明：本阶段论文精读的视频课分为两小节，第一节首先介绍了论文的整体框架，这部分我们根据论文结构和摘要理清作者的写研究和写作思路；然后讲解了神经网络语言模型NNLM，这部分我们详细阐述了如何利用神经网络语言模型生成分布式表示的词向量，为之后学习word2vec的两种模型打下基础。第二节带领大家详细学习论文提出的word2vec的两种改进模型CBOW和skip-gram，详细讲解了如何两种模型的原理以及如何利用这两种模型生成词向量。之后讲解了论文实验中的一些设置和并且分析了实验结果。最后我们进行了论文的讨论和总结，并展望该领域未来的发展方向。

这部分主要是论文的理论学习部分，要求大家结合视频课程再次精读论文，搞懂在预习时留下的问题，争取做到没有知识盲点。在学习完本阶段后要明白word2vec工具中的两种模型与传统神经网络模型的区别，为什么要这样做，优点在哪，除此之外还有什么改进的方法。及时做好学习笔记。

打卡要求：（形式：文字+图片，字数不限）

1. 分别画出神经网络语言模型、CBOW和Skip-gram模型的数据流程图。

2. 根据实验结果分析为什么论文中表4中skip-gram比CBOW的模型效果好。

3. 列举三个学习完本课程的问题。



### 2.3 第三课时

<video width=80%  controls >
	<source type="video/mp4" src="002-efficient-estimation-of-word-representations-in-vector-space/002-3.mp4">
</video>

任务名称：论文精读，学习论文的算法原理，掌握算法的流程和实现方式

任务简介：精读整篇论文，学习视频第二课时-论文精读部分

详细说明：本阶段论文精读的视频课分为两小节，第一节首先介绍了论文的整体框架，这部分我们根据论文结构和摘要理清作者的写研究和写作思路；然后讲解了神经网络语言模型NNLM，这部分我们详细阐述了如何利用神经网络语言模型生成分布式表示的词向量，为之后学习word2vec的两种模型打下基础。第二节带领大家详细学习论文提出的word2vec的两种改进模型CBOW和skip-gram，详细讲解了如何两种模型的原理以及如何利用这两种模型生成词向量。之后讲解了论文实验中的一些设置和并且分析了实验结果。最后我们进行了论文的讨论和总结，并展望该领域未来的发展方向。

这部分主要是论文的理论学习部分，要求大家结合视频课程再次精读论文，搞懂在预习时留下的问题，争取做到没有知识盲点。在学习完本阶段后要明白word2vec工具中的两种模型与传统神经网络模型的区别，为什么要这样做，优点在哪，除此之外还有什么改进的方法。及时做好学习笔记。

打卡要求：（形式：文字+图片，字数不限）

1. 分别画出神经网络语言模型、CBOW和Skip-gram模型的数据流程图。

2. 根据实验结果分析为什么论文中表4中skip-gram比CBOW的模型效果好。

3. 列举三个学习完本课程的问题。



### 2.4 第四课时

<video width=80%  controls >
	<source type="video/mp4" src="002-efficient-estimation-of-word-representations-in-vector-space/002-4.mp4">
</video>

任务名称：代码精读，学习论文源码的细节，掌握论文思想的实现方式。

任务简介：下载相关数据和代码，学习视频第三课时-代码精读部分

详细说明：本节视频课主要讲解论文思想的代码实现过程，首先介绍了运行代码之前的准备工作，包括代码的数据获取方式和运行环境等，之后主要针对skip-gram模型的思想详细讲解了实现过程。此外，对比skip-gram模型与CBOW模型的不同，还简单介绍了CBOW模型的部分实现细节。对代码的运行结果给了必要的分析和展示。

本阶段主要是论文思想的实战部分，要求大家结合视频课程运行代码，搞懂代码各部分组成以及功能是什么。在调试代码时可能遇到很多问题，要求同学们耐心解决，知道达到想要的结果。学习完本节口争取能自己构建一个神经机器翻译模型。

代码下载链接：

链接：https://pan.baidu.com/s/1RvlAYOt07Swm6lasf2Ra-g 

提取码：i5p3 

打卡要求：（形式：文字+图片，字数不限）

1.运行代码，对运行结果截图。

2.写出你在运行代码时遇到的问题。

3.列举三个学习完本课程的问题。

4.提出几个你对当前实现代码的改进想法。