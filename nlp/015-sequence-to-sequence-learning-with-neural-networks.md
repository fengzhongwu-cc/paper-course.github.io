## 15. Sequence to Sequence Learning with Neural Networks

### 15.1 第一课时

<video width=80%  controls >
	<source type="video/mp4" src="015-sequence-to-sequence-learning-with-neural-networks/015-1.mp4">
</video>

**任务名称：**了解神经机器翻译的概念以及两种神经机器翻译模型。了解LSTM以及多层LSTM的概念以及Seq2Seq模型。

**任务简介：**观看视频第一课时，阅读论文

**任务详解：**

1. 视频第一课时

   1. 神经机器翻译概率

      神经机器翻译就是使用神经网络使得机器能够自动将一种语言的句子翻译成另外一种语言的句子，它可以解决不同母语的人之间的交流障碍。

   2. 两种神经机器翻译模型

      最开始的神经机器翻译模型只是使用一个多层的LSTM，多层LSTM的最底层为源语言的输入，多层LSTM的最高层为目标语言的输出。之后产生了Encoder-Decoder的模型，即Encoder将源语言的句子压缩成一个向量，Decoder利用压缩得到的向量生成目标语言的句子。

3. LSTM以及多层LSTM

   LSTM是一种特殊的RNN，通过增加一个记忆细胞和几个门来加强序列处理的长距离依赖。而多层LSTM就是单层LSTM网上叠加，上一层LSTM每一个时间步的输出作为当前层LSTM的输入。

2. 论文阅读

   花费大致2h来阅读论文，主要通过阅读了解模型的大致结构，将疑问记录在作业中。

   这个部分我们只需要比较熟悉地了解到导读视频的程度，不需要过于深入地去学习，将重点放置在论文的模型上，对于其他的部分有疑问可以暂时记录着，并写在这两天的作业中。

**论文原文及代码下载链接：**

链接：https://pan.baidu.com/s/13p-YElNGPAKdWfJHfCEtoA 

提取码：w7rb 

打卡要求：（形式：文字，字数至少100）总结两种神经机器翻译模型的联系和区别，试图画图解释单层LSTM与多层LSTM。

### 15.2 第二课时

<video width=80%  controls >
	<source type="video/mp4" src="015-sequence-to-sequence-learning-with-neural-networks/015-2.mp4">
</video>

**任务名称：**进一步了解论文中的基于多层LSTM的神经机器翻译模型的部分细节，从理论掌握模型结构，论文中的技巧

**任务简介：**观看视频第二课时，阅读论文

**任务详解：**

1. 视频

   1. 上节回顾

      回顾上节讲的神经机器翻译的概念以及两种神经机器翻译模型。

   2. 论文中提到的对比模型

      理解论文中提到了基于attention的神经机器翻译模型。

   3. 论文提出的模型以及实验结果

      深入理解论文提出的基于多层LSTM的神经机器翻译模型，并查看实验结果。

2. 阅读论文

   先自己尝试观看本次论文，然后在看视频进一步解决自己的困惑并加强理解本篇论文。

**打卡要求：**（形式：文字或者图片，字数至少100）

1. 本文为何使用多层LSTM来代替单层LSTM？

2. 本文创新点是什么？

3. 本文为何对输入进行逆序？

### 15.3 第三课时

<video width=80%  controls >
	<source type="video/mp4" src="015-sequence-to-sequence-learning-with-neural-networks/015-3.mp4">
</video>

**任务名称：**代码实践的方法与流程，讲解论文提出的模型的tensorflow实现

**任务简介：**观看视频第三课时，跑通提供的代码，调整网络结构提升准确率

**任务详解：**

1. 视频

   1. 数据处理方法

      本文的数据处理是针对机器翻译的双语预料，希望大家学习双语预料的数据处理方法。

   2. 模型的构建

      使用tensorflow构建整个模型，理解其中针对Decoder和Beam Search的一系列操作背后的原理。

   3. 训练和测试

      利用构建好的模型在以及处理好的数据上进行训练和测试并验证试验结果。

2. 调整学习轮次提高准确率

   通过调整学习的轮次查看模型的准确率是否能够一直提高。

**打卡要求：**（形式：图片）

1. 开源自己的tensorflow (其他框架均可以)实现。

2. 在iwslt14数据集上的实验结果并截图。

3. 试图提高模型在测试集上的结果。